{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503df2c9-23c7-46f4-8e1a-c1a2f66eade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Data Loading Functions\n",
    "# -----------------------------\n",
    "def load_combined_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load combined CSV files for training and testing.\n",
    "    Debugging: Prints number of rows and columns for each dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"[DEBUG] Training data loaded: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "        print(f\"[DEBUG] Test data loaded: {test_df.shape[0]} rows, {test_df.shape[1]} columns\")\n",
    "        return train_df, test_df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load data: {e}\")\n",
    "        raise\n",
    "\n",
    "def load_rul_data(file_path):\n",
    "    \"\"\"\n",
    "    Load RUL (Remaining Useful Life) data from a text file.\n",
    "    Debugging: Prints error if file cannot be loaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rul_df = pd.read_csv(file_path, sep=' ', header=None)\n",
    "        rul_df = rul_df.dropna(axis=1)\n",
    "        rul_df.columns = ['RUL']\n",
    "        print(f\"[DEBUG] RUL data loaded with {rul_df.shape[0]} rows.\")\n",
    "        return rul_df\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error loading RUL file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Feature Engineering Functions\n",
    "# -----------------------------\n",
    "def add_rul(df):\n",
    "    \"\"\"\n",
    "    Calculate and add the Remaining Useful Life (RUL) for each engine unit.\n",
    "    Debugging: Prints unique source files processed.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    unique_sources = df['source_file'].unique()\n",
    "    print(f\"[DEBUG] Processing RUL for source files: {unique_sources}\")\n",
    "    \n",
    "    for source_file in unique_sources:\n",
    "        df_subset = df[df['source_file'] == source_file].copy()\n",
    "        # Get maximum cycle for each unit (assumed failure point in training data)\n",
    "        max_cycles = df_subset.groupby('unit')['cycle'].max().reset_index()\n",
    "        max_cycles.columns = ['unit', 'max_cycle']\n",
    "        df_subset = df_subset.merge(max_cycles, on='unit', how='left')\n",
    "        df_subset['RUL'] = df_subset['max_cycle'] - df_subset['cycle']\n",
    "        df_subset.drop('max_cycle', axis=1, inplace=True)\n",
    "        dfs.append(df_subset)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"[DEBUG] RUL added. Data now has {combined_df.shape[1]} columns.\")\n",
    "    return combined_df\n",
    "\n",
    "def add_failure_indicator(df, threshold=30):\n",
    "    \"\"\"\n",
    "    Add a binary failure indicator: 1 if RUL <= threshold, else 0.\n",
    "    Debugging: Print the distribution of the new indicator.\n",
    "    \"\"\"\n",
    "    df['failure_imminent'] = (df['RUL'] <= threshold).astype(int)\n",
    "    print(f\"[DEBUG] Failure indicator added (threshold={threshold}). Distribution:\\n{df['failure_imminent'].value_counts()}\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Preprocessing Functions\n",
    "# -----------------------------\n",
    "def preprocess_data(df, scaler=None, is_training=True):\n",
    "    \"\"\"\n",
    "    Preprocess sensor and operational features:\n",
    "      - Convert to numeric and scale features.\n",
    "    Debugging: Print shape of scaled features.\n",
    "    \"\"\"\n",
    "    # Identify sensor and operational setting columns\n",
    "    sensor_cols = [col for col in df.columns if col.startswith('sensor')]\n",
    "    op_cols = [col for col in df.columns if col.startswith('op_set')]\n",
    "    feature_cols = op_cols + sensor_cols\n",
    "\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert feature columns to numeric\n",
    "    for col in feature_cols:\n",
    "        df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')\n",
    "    \n",
    "    # Scale features\n",
    "    if scaler is None and is_training:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(df_copy[feature_cols])\n",
    "        print(f\"[DEBUG] Features scaled (training). Shape: {X_scaled.shape}\")\n",
    "    else:\n",
    "        X_scaled = scaler.transform(df_copy[feature_cols])\n",
    "        print(f\"[DEBUG] Features scaled (inference). Shape: {X_scaled.shape}\")\n",
    "    \n",
    "    # Build a new DataFrame with scaled features\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "    # Reattach metadata columns\n",
    "    for col in ['unit', 'cycle', 'RUL', 'failure_imminent', 'source_file']:\n",
    "        if col in df_copy.columns:\n",
    "            X_scaled_df[col] = df_copy[col].values\n",
    "    \n",
    "    return (X_scaled_df, scaler) if is_training else X_scaled_df\n",
    "\n",
    "def reduce_dimensions(df, pca=None, n_components=10, is_training=True):\n",
    "    \"\"\"\n",
    "    Reduce dimensions using PCA on sensor and operational features.\n",
    "    Debugging: Print explained variance ratio.\n",
    "    \"\"\"\n",
    "    feature_cols = [col for col in df.columns if col.startswith('sensor') or col.startswith('op_set')]\n",
    "    \n",
    "    if pca is None and is_training:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(df[feature_cols])\n",
    "        print(f\"[DEBUG] PCA fitted. Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    else:\n",
    "        X_pca = pca.transform(df[feature_cols])\n",
    "    \n",
    "    X_pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(n_components)])\n",
    "    \n",
    "    # Reattach metadata columns\n",
    "    for col in ['unit', 'cycle', 'RUL', 'failure_imminent', 'source_file']:\n",
    "        if col in df.columns:\n",
    "            X_pca_df[col] = df[col].values\n",
    "    \n",
    "    return (X_pca_df, pca) if is_training else X_pca_df\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Sequence Preparation for LSTM\n",
    "# -----------------------------\n",
    "def prepare_sequences(df, sequence_length=30, step=1):\n",
    "    \"\"\"\n",
    "    Convert time-series data into sequences for LSTM.\n",
    "    Debugging: Print the number of sequences created.\n",
    "    \"\"\"\n",
    "    feature_cols = [col for col in df.columns if col.startswith('PC')]\n",
    "    metadata_cols = ['unit', 'cycle', 'RUL', 'failure_imminent', 'source_file']\n",
    "    metadata_cols = [col for col in metadata_cols if col in df.columns]\n",
    "    \n",
    "    sequences = []\n",
    "    metadata = []\n",
    "    \n",
    "    for source_file in df['source_file'].unique():\n",
    "        df_dataset = df[df['source_file'] == source_file]\n",
    "        for unit in df_dataset['unit'].unique():\n",
    "            unit_data = df_dataset[df_dataset['unit'] == unit].sort_values('cycle')\n",
    "            if len(unit_data) >= sequence_length:\n",
    "                features = unit_data[feature_cols].values\n",
    "                unit_metadata = unit_data[metadata_cols].values\n",
    "                for i in range(0, len(features) - sequence_length + 1, step):\n",
    "                    sequences.append(features[i:i+sequence_length])\n",
    "                    metadata.append(unit_metadata[i+sequence_length-1])\n",
    "    \n",
    "    sequences_arr = np.array(sequences)\n",
    "    metadata_arr = np.array(metadata)\n",
    "    print(f\"[DEBUG] Prepared {sequences_arr.shape[0]} sequences of length {sequence_length}.\")\n",
    "    return sequences_arr, metadata_arr\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Define LSTM Autoencoder Model\n",
    "# -----------------------------\n",
    "class LSTMAutoencoder:\n",
    "    def __init__(self, input_dim, latent_dim=16, timesteps=30):\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.timesteps = timesteps\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "    def _build_model(self):\n",
    "        # Build encoder\n",
    "        inputs = Input(shape=(self.timesteps, self.input_dim))\n",
    "        encoded = LSTM(32, activation='relu', return_sequences=True)(inputs)\n",
    "        encoded = LSTM(self.latent_dim, activation='relu')(encoded)\n",
    "        \n",
    "        # Build decoder\n",
    "        decoded = RepeatVector(self.timesteps)(encoded)\n",
    "        decoded = LSTM(32, activation='relu', return_sequences=True)(decoded)\n",
    "        decoded = LSTM(self.input_dim, activation='relu', return_sequences=True)(decoded)\n",
    "        \n",
    "        autoencoder = Model(inputs, decoded)\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        print(\"[DEBUG] LSTM Autoencoder model built.\")\n",
    "        return autoencoder\n",
    "    \n",
    "    def fit(self, X, validation_data=None, epochs=50, batch_size=32):\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss' if validation_data is not None else 'loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        history = self.model.fit(\n",
    "            X, X,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(validation_data, validation_data) if validation_data is not None else None,\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def compute_anomaly_scores(self, X, threshold=None):\n",
    "        X_pred = self.predict(X)\n",
    "        mse = np.mean(np.power(X - X_pred, 2), axis=(1, 2))\n",
    "        if threshold is None:\n",
    "            threshold = np.percentile(mse, 95)\n",
    "        anomalies = (mse > threshold).astype(int)\n",
    "        return anomalies, mse, threshold\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Main Function â€“ Putting Everything Together\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # File paths: update with your actual file locations\n",
    "    combined_train_path = \"combined_cmapps_training.csv\"\n",
    "    combined_test_path = \"combined_cmapps_dataset.csv\"\n",
    "    \n",
    "    print(\"[INFO] Loading combined datasets...\")\n",
    "    combined_train, combined_test = load_combined_data(combined_train_path, combined_test_path)\n",
    "    \n",
    "    # Check required column\n",
    "    for df in [combined_train, combined_test]:\n",
    "        if 'source_file' not in df.columns:\n",
    "            print(\"[ERROR] 'source_file' column is missing. Exiting.\")\n",
    "            return\n",
    "    \n",
    "    print(\"[INFO] Processing training data: Adding RUL and failure indicators...\")\n",
    "    train_df_with_rul = add_rul(combined_train)\n",
    "    train_df_with_indicators = add_failure_indicator(train_df_with_rul)\n",
    "    \n",
    "    print(\"[INFO] Preprocessing training data...\")\n",
    "    X_train_scaled, scaler = preprocess_data(train_df_with_indicators, is_training=True)\n",
    "    \n",
    "    print(\"[INFO] Reducing dimensions using PCA...\")\n",
    "    X_train_pca, pca = reduce_dimensions(X_train_scaled, n_components=10, is_training=True)\n",
    "    \n",
    "    sequence_length = 30\n",
    "    print(f\"[INFO] Preparing sequences (length={sequence_length}) for training...\")\n",
    "    X_sequences, X_metadata = prepare_sequences(X_train_pca, sequence_length=sequence_length)\n",
    "    \n",
    "    # Debug: Check sequence shapes\n",
    "    print(f\"[DEBUG] Training sequences shape: {X_sequences.shape}\")\n",
    "    \n",
    "    # Split training sequences for validation (80/20 split)\n",
    "    split_idx = int(0.8 * len(X_sequences))\n",
    "    X_train_seq, X_val_seq = X_sequences[:split_idx], X_sequences[split_idx:]\n",
    "    print(f\"[DEBUG] Training set: {X_train_seq.shape}, Validation set: {X_val_seq.shape}\")\n",
    "    \n",
    "    print(\"[INFO] Training LSTM Autoencoder...\")\n",
    "    input_dim = X_train_seq.shape[2]\n",
    "    lstm_ae = LSTMAutoencoder(input_dim=input_dim, latent_dim=8, timesteps=sequence_length)\n",
    "    history = lstm_ae.fit(X_train_seq, validation_data=X_val_seq, epochs=30, batch_size=32)\n",
    "    \n",
    "    # Save the trained model for later use\n",
    "    model_save_path = 'lstm_autoencoder.h5'\n",
    "    lstm_ae.model.save(model_save_path)\n",
    "    print(f\"[INFO] Model saved to '{model_save_path}'.\")\n",
    "    \n",
    "    # Visualize training history\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute source-specific thresholds on validation data\n",
    "    print(\"[INFO] Computing source-specific anomaly thresholds...\")\n",
    "    def compute_source_thresholds(model, sequences, metadata):\n",
    "        thresholds = {}\n",
    "        _, mse, _ = model.compute_anomaly_scores(sequences)\n",
    "        source_file_col_idx = -1  # assuming the last column of metadata is 'source_file'\n",
    "        for source_file in np.unique(metadata[:, source_file_col_idx]):\n",
    "            source_indices = metadata[:, source_file_col_idx] == source_file\n",
    "            source_mse = mse[source_indices]\n",
    "            if len(source_mse) > 0:\n",
    "                thresholds[source_file] = np.percentile(source_mse, 95)\n",
    "        return thresholds\n",
    "\n",
    "    source_thresholds = compute_source_thresholds(lstm_ae, X_val_seq, X_metadata)\n",
    "    for src, thr in source_thresholds.items():\n",
    "        print(f\"[DEBUG] Source '{src}' threshold: {thr:.6f}\")\n",
    "    \n",
    "    print(\"[INFO] Processing test data...\")\n",
    "    test_df_with_rul = add_rul(combined_test)\n",
    "    test_df_with_indicators = add_failure_indicator(test_df_with_rul)\n",
    "    X_test_scaled = preprocess_data(test_df_with_indicators, scaler=scaler, is_training=False)\n",
    "    X_test_pca = reduce_dimensions(X_test_scaled, pca=pca, is_training=False)\n",
    "    X_test_sequences, X_test_metadata = prepare_sequences(X_test_pca, sequence_length=sequence_length)\n",
    "    print(f\"[DEBUG] Created {len(X_test_sequences)} test sequences\")\n",
    "    \n",
    "    print(\"[INFO] Detecting anomalies in test data...\")\n",
    "    all_anomalies, all_mse, _ = lstm_ae.compute_anomaly_scores(X_test_sequences)\n",
    "    print(f\"[RESULT] Total anomalies detected in test data: {all_anomalies.sum()} out of {len(all_anomalies)} sequences\")\n",
    "    \n",
    "    # Visualize reconstruction error distribution on test data\n",
    "    plt.figure()\n",
    "    sns.kdeplot(all_mse, label='Test MSE')\n",
    "    plt.title('Reconstruction Error Distribution on Test Data')\n",
    "    plt.xlabel('MSE')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37035a9d-1180-4128-9716-b0913e7532ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
