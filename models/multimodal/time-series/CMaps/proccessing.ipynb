{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cb42b9-8986-4144-91fc-c7e8b79ac68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0x96 in position 707: invalid start byte\n",
      "Expected 26 columns, but found 1 in RUL_FD001.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD002.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD003.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD004.txt.\n",
      "Expected 26 columns, but found 1 in x.txt.\n",
      "Combined dataset saved as 'combined_cmapps_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_cmapps_file(file_path, fault_mode, condition):\n",
    "    \"\"\"\n",
    "    Process a raw CMAPSS file and return a DataFrame with proper column names and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the raw text file.\n",
    "      fault_mode (str): Fault mode description.\n",
    "      condition (str): Operating condition description.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the file (assumed whitespace-separated with no header)\n",
    "    df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "    \n",
    "    # Check if file has 26 columns\n",
    "    if df.shape[1] != 26:\n",
    "        raise ValueError(f\"Expected 26 columns, but found {df.shape[1]} in {file_path}.\")\n",
    "    \n",
    "    # Define column names based on dataset documentation\n",
    "    col_names = ['unit', 'cycle'] + ['op_set1', 'op_set2', 'op_set3'] + \\\n",
    "                [f'sensor{i}' for i in range(1, 22)]\n",
    "    df.columns = col_names\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df['fault_mode'] = fault_mode\n",
    "    df['condition'] = condition\n",
    "    # Also add the source file name for traceability\n",
    "    df['source_file'] = os.path.basename(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define mapping for each dataset file (customize these as needed)\n",
    "# For demonstration, assume files are named FD001.txt, FD002.txt, etc.\n",
    "file_info = {\n",
    "    'test_FD001.txt': {'fault_mode': 'HPC Degradation', 'condition': 'Sea Level'},\n",
    "    'test_FD002.txt': {'fault_mode': 'HPC Degradation', 'condition': 'SIX'},\n",
    "    'test_FD003.txt': {'fault_mode': 'HPC & Fan Degradation', 'condition': 'Sea Level'},\n",
    "    'test_FD004.txt': {'fault_mode': 'HPC & Fan Degradation', 'condition': 'SIX'},\n",
    "}\n",
    "\n",
    "# Use glob to find all relevant files in a folder\n",
    "file_list = glob.glob('*.txt')  # Adjust the pattern or path as needed\n",
    "\n",
    "# List to hold processed DataFrames\n",
    "dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    # Use mapping if available; otherwise, set defaults\n",
    "    info = file_info.get(os.path.basename(file), {'fault_mode': 'Unknown', 'condition': 'Unknown'})\n",
    "    try:\n",
    "        df_temp = process_cmapps_file(file, fault_mode=info['fault_mode'], condition=info['condition'])\n",
    "        dfs.append(df_temp)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Save to CSV\n",
    "    final_df.to_csv('combined_cmapps_dataset.csv', index=False)\n",
    "    print(\"Combined dataset saved as 'combined_cmapps_dataset.csv'.\")\n",
    "else:\n",
    "    print(\"No valid files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c92656c1-6863-4d0b-aa93-1b995d7772cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('combined_cmapps_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9850fc43-7f2d-4f30-bb4c-e02ebbeb4575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unit', 'cycle', 'op_set1', 'op_set2', 'op_set3', 'sensor1', 'sensor2',\n",
       "       'sensor3', 'sensor4', 'sensor5', 'sensor6', 'sensor7', 'sensor8',\n",
       "       'sensor9', 'sensor10', 'sensor11', 'sensor12', 'sensor13', 'sensor14',\n",
       "       'sensor15', 'sensor16', 'sensor17', 'sensor18', 'sensor19', 'sensor20',\n",
       "       'sensor21', 'fault_mode', 'condition', 'source_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e007cd31-55eb-4fd4-98ad-763a8a04cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'utf-8' codec can't decode byte 0x96 in position 707: invalid start byte\n",
      "Expected 26 columns, but found 1 in RUL_FD001.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD002.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD003.txt.\n",
      "Expected 26 columns, but found 1 in RUL_FD004.txt.\n",
      "Expected 26 columns, but found 1 in x.txt.\n",
      "Combined dataset saved as 'combined_cmapps_training.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_cmapps_file(file_path, fault_mode, condition):\n",
    "    \"\"\"\n",
    "    Process a raw CMAPSS file and return a DataFrame with proper column names and metadata.\n",
    "    \n",
    "    Parameters:\n",
    "      file_path (str): Path to the raw text file.\n",
    "      fault_mode (str): Fault mode description.\n",
    "      condition (str): Operating condition description.\n",
    "      \n",
    "    Returns:\n",
    "      pd.DataFrame: Processed DataFrame.\n",
    "    \"\"\"\n",
    "    # Read the file (assumed whitespace-separated with no header)\n",
    "    df = pd.read_csv(file_path, sep='\\s+', header=None)\n",
    "    \n",
    "    # Check if file has 26 columns\n",
    "    if df.shape[1] != 26:\n",
    "        raise ValueError(f\"Expected 26 columns, but found {df.shape[1]} in {file_path}.\")\n",
    "    \n",
    "    # Define column names based on dataset documentation\n",
    "    col_names = ['unit', 'cycle'] + ['op_set1', 'op_set2', 'op_set3'] + \\\n",
    "                [f'sensor{i}' for i in range(1, 22)]\n",
    "    df.columns = col_names\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df['fault_mode'] = fault_mode\n",
    "    df['condition'] = condition\n",
    "    # Also add the source file name for traceability\n",
    "    df['source_file'] = os.path.basename(file_path)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define mapping for each dataset file (customize these as needed)\n",
    "# For demonstration, assume files are named FD001.txt, FD002.txt, etc.\n",
    "file_info = {\n",
    "    'train_FD001.txt': {'fault_mode': 'HPC Degradation', 'condition': 'Sea Level'},\n",
    "    'train_FD002.txt': {'fault_mode': 'HPC Degradation', 'condition': 'SIX'},\n",
    "    'train_FD003.txt': {'fault_mode': 'HPC & Fan Degradation', 'condition': 'Sea Level'},\n",
    "    'train_FD004.txt': {'fault_mode': 'HPC & Fan Degradation', 'condition': 'SIX'},\n",
    "}\n",
    "\n",
    "# Use glob to find all relevant files in a folder\n",
    "file_list = glob.glob('*.txt')  # Adjust the pattern or path as needed\n",
    "\n",
    "# List to hold processed DataFrames\n",
    "dfs = []\n",
    "\n",
    "for file in file_list:\n",
    "    # Use mapping if available; otherwise, set defaults\n",
    "    info = file_info.get(os.path.basename(file), {'fault_mode': 'Unknown', 'condition': 'Unknown'})\n",
    "    try:\n",
    "        df_temp = process_cmapps_file(file, fault_mode=info['fault_mode'], condition=info['condition'])\n",
    "        dfs.append(df_temp)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Save to CSV\n",
    "    final_df.to_csv('combined_cmapps_training.csv', index=False)\n",
    "    print(\"Combined dataset saved as 'combined_cmapps_training.csv'.\")\n",
    "else:\n",
    "    print(\"No valid files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5662e-6718-4399-9896-e71e40cd2e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
